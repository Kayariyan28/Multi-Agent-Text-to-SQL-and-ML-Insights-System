# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qLgpM_Lq5OeLFCMZw8bIQ5VEt-u3B2Su
"""

# @title # 1. Installation and Setup
# @markdown This section installs all necessary libraries, configures the multiple LLM providers, and sets up the environment, including Spark for ML tasks.

# --- Installation ---
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz
!tar xf spark-3.5.1-bin-hadoop3.tgz
!wget -q https://repo1.maven.org/maven2/com/google/cloud/spark-bigquery-with-dependencies_2.12/0.37.0/spark-bigquery-with-dependencies_2.12-0.37.0
!pip install -qU langchain langchain_google_genai langgraph google-cloud-aiplatform pandas db-dtypes google-auth google-auth-oauthlib google-auth-httplib2 langchain-openai langchain-anthropic findspark pyspark

# --- Authentication and Project Setup ---
import os
import sys
import getpass
from google.colab import auth
import google.auth
from google.cloud import bigquery
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain.prompts import PromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langgraph.graph import StateGraph, END
from typing import TypedDict, Optional
import pandas as pd
import findspark
from pyspark.sql import SparkSession

# --- API Key Configuration ---
os.environ['GOOGLE_API_KEY'] = getpass.getpass('Enter your Google API Key: ')
os.environ['OPENAI_API_KEY'] = getpass.getpass('Enter your OpenAI API Key: ')
os.environ['ANTHROPIC_API_KEY'] = getpass.getpass('Enter your Anthropic API Key: ')
os.environ['XAI_API_KEY'] = getpass.getpass('Enter your xAI API Key: ')

# --- Google Cloud Authentication ---
auth.authenticate_user()

# Get project ID
if 'google.colab' in sys.modules:
    project_id = os.environ.get('GOOGLE_CLOUD_PROJECT')
    if not project_id:
        try:
            project_id = !gcloud config get-value project
            project_id = project_id[0]
            print(f"gcloud project_id: {project_id}")
        except:
            print("Could not retrieve project ID. Please set it manually.")
            project_id = "your-gcp-project-id" # @param {type:"string"}
else:
    project_id = "your-gcp-project-id" # @param {type:"string"}

os.environ['GOOGLE_CLOUD_PROJECT'] = project_id
google.auth.default()

# --- BigQuery Client ---
BQ_DATASET = "your_bq_dataset"  # @param {type:"string"}
BQ_TABLE = "your_bq_table"      # @param {type:"string"}
LOCATION = "US" # Or your BigQuery dataset location

bq_client = bigquery.Client(project=project_id, location=LOCATION)

# --- Spark Session for ML Tasks ---
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.5.1-bin-hadoop3"
findspark.init()
spark = SparkSession.builder \
    .master("local[*]") \
    .appName("BigQuery ML Spark") \
    .config("spark.jars", "/content/spark-bigquery-with-dependencies_2.12-0.37.0.jar") \
    .getOrCreate()

print("Spark session initialized for ML tasks.")

# --- Initialize specialized LLM clients ---
# Agent 1 (Code Generator): Gemini 1.5 Pro for advanced code generation (SQL and ML)
llm_coder = ChatGoogleGenerativeAI(model="gemini-1.5-pro", temperature=0.0, convert_system_message_to_human=True)

# Agent 2 (Planner): OpenAI o1-preview for strategic planning and task type determination
llm_planner = ChatOpenAI(model="o1-preview", temperature=0.0)

# Agent 3 (Code Reviewer): xAI Grok for logical validation and refinement
llm_reviewer = ChatOpenAI(
    model="grok-beta",
    api_key=os.environ["XAI_API_KEY"],
    base_url="https://api.x.ai/v1",
    temperature=0.0
)

# Agent 4 (Insight Analyst): Anthropic Claude 3.5 Sonnet for nuanced insight generation
llm_analyst = ChatAnthropic(model="claude-3-5-sonnet-20240620", temperature=0.1)

print("âœ… Setup complete. All models and environments are initialized and ready.")


# @title # 2. Schema and Helper Functions
# @markdown These functions retrieve the BigQuery table schema and execute code (SQL or PySpark).

def get_table_schema(client, dataset, table):
    """Retrieves the schema of a BigQuery table and formats it for the LLM."""
    try:
        table_ref = client.dataset(dataset).table(table)
        table = client.get_table(table_ref)
        schema_str = "Table Schema:\n"
        for schema_field in table.schema:
            schema_str += f"- {schema_field.name}: {schema_field.field_type}\n"
        return schema_str
    except Exception as e:
        return f"Error retrieving schema: {e}"

def execute_sql_query(query: str) -> pd.DataFrame:
    """Executes a BigQuery SQL query and returns the result as a Pandas DataFrame."""
    try:
        print("Executing BigQuery SQL query...")
        query_job = bq_client.query(query)
        results = query_job.to_dataframe()
        print("SQL query executed successfully.")
        return results
    except Exception as e:
        print(f"Error executing BigQuery query: {e}")
        return pd.DataFrame()

def execute_pyspark_code(code: str) -> pd.DataFrame:
    """Executes PySpark code and captures the result_df as a Pandas DataFrame."""
    try:
        print("Executing PySpark ML code...")
        locals_dict = {'spark': spark, 'BQ_DATASET': BQ_DATASET, 'BQ_TABLE': BQ_TABLE, 'project_id': project_id}
        exec(code, globals(), locals_dict)
        result_df = locals_dict.get('result_df')
        if result_df is None:
            raise ValueError("No 'result_df' defined in the code.")
        print("PySpark code executed successfully.")
        return result_df.toPandas()
    except Exception as e:
        print(f"Error executing PySpark code: {e}")
        return pd.DataFrame()


# @title # 3. Agent State Definition
# @markdown This defines the `State` object passed between nodes.

class AgentState(TypedDict):
    """
    Represents the state of our multi-agent system.
    """
    question: str
    schema: str
    task_type: str  # 'sql' or 'ml'
    plan: str
    initial_code: str
    refined_code: str
    result_df: pd.DataFrame
    insight: str
    error: Optional[str]


# @title # 4. The Specialized Agent Nodes
# @markdown Core logic for the specialized agents, including branching for SQL/ML.

# --- Agent 2: Planning Agent (OpenAI o1-preview) ---
def planning_agent(state: AgentState) -> AgentState:
    """
    Determines task type (SQL or ML) and generates a step-by-step plan.
    """
    print("---AGENT 2: Planner (OpenAI o1-preview)---")
    prompt_template = """
    You are a master strategist and business analyst at McKinsey level. First, analyze the user's question to determine if it can be answered with a standard SQL query or if it requires advanced ML for deeper insights (e.g., prediction, clustering, classification, anomaly detection).

    - Use SQL for data retrieval, aggregation, filtering, joins, etc.
    - Use ML for predictive modeling, pattern discovery, recommendations, etc. You can use SparkML for ML tasks.

    Output format:
    Type: SQL or ML

    Step-by-Step Plan:
    [Detailed steps to answer the question, including required data, calculations, or ML algorithms]

    **Schema:**
    {schema}

    **User Question:**
    {question}
    """
    prompt = PromptTemplate.from_template(prompt_template)
    chain = prompt | llm_planner | StrOutputParser()
    response = chain.invoke({"schema": state['schema'], "question": state['question']})

    # Parse response
    lines = response.split('\n')
    task_type = lines[0].split(': ')[1].strip().lower()
    plan = '\n'.join(lines[1:]).strip()

    print(f"Determined Task Type: {task_type.upper()}")
    print(f"Generated Plan:\n{plan}")
    return {"task_type": task_type, "plan": plan}

# --- Agent 1: SQL Generation Agent (Gemini 1.5 Pro) ---
def sql_generation_agent(state: AgentState) -> AgentState:
    """
    Generates advanced BigQuery SQL query based on the plan.
    """
    print("---AGENT 1: SQL Coder (Gemini 1.5 Pro)---")
    prompt_template = """
    You are a Google BigQuery expert. Write a single, advanced BigQuery SQL query to execute the plan. Use CTEs, window functions, etc., for deep analysis.
    **DO NOT** wrap in markdown. Output only the raw SQL query.

    **Schema:** {schema}
    **Plan:** {plan}
    """
    prompt = PromptTemplate.from_template(prompt_template)
    chain = prompt | llm_coder | StrOutputParser()
    code = chain.invoke({"schema": state['schema'], "plan": state['plan']})
    print(f"Generated SQL:\n{code}")
    return {"initial_code": code}

# --- Agent 1: ML Generation Agent (Gemini 1.5 Pro) ---
def ml_generation_agent(state: AgentState) -> AgentState:
    """
    Generates PySpark code using SparkML based on the plan.
    """
    print("---AGENT 1: ML Coder (Gemini 1.5 Pro)---")
    prompt_template = """
    You are a SparkML expert. Write PySpark code to execute the ML plan. Read data from BigQuery using spark.read.format("bigquery").options(table=f"{project_id}.{BQ_DATASET}.{BQ_TABLE}").load().
    Use SparkML libraries for modeling. End with: result_df = [final DataFrame with insights]
    Import necessary modules. Assume spark is available.

    **Schema:** {schema}
    **Plan:** {plan}
    """
    prompt = PromptTemplate.from_template(prompt_template)
    chain = prompt | llm_coder | StrOutputParser()
    code = chain.invoke({"schema": state['schema'], "plan": state['plan']})
    print(f"Generated PySpark Code:\n{code}")
    return {"initial_code": code}

# --- Agent 3: Code Reviewing Agent (xAI Grok) ---
def code_reviewing_agent(state: AgentState) -> AgentState:
    """
    Reviews and refines SQL or PySpark code for correctness and efficiency.
    """
    print("---AGENT 3: Code Reviewer (xAI Grok)---")
    prompt_template = """
    You are a senior code reviewer. Review the {task_type} code for syntax, logic, and efficiency.
    Verify it implements the question correctly.
    If perfect, output the original code.
    If improvements needed, output the refined code only. No explanations.

    **Question:** {question}
    **Code to Review:** {code}
    """
    prompt = PromptTemplate.from_template(prompt_template)
    chain = prompt | llm_reviewer | StrOutputParser()
    refined_code = chain.invoke({"task_type": state['task_type'].upper(), "question": state['question'], "code": state['initial_code']})
    print(f"Refined Code:\n{refined_code}")
    return {"refined_code": refined_code}

# --- Agent 4: Insight Analyst Agent (Claude 3.5 Sonnet) ---
def insight_analyst_agent(state: AgentState) -> AgentState:
    """
    Generates deep business insights from the result.
    """
    print("---AGENT 4: Insight Analyst (Claude 3.5 Sonnet)---")
    if state['result_df'].empty:
        return {"insight": "No data returned. Cannot generate insights.", "error": "Empty result."}

    result_str = state['result_df'].to_string()
    prompt_template = """
    You are a McKinsey-level business consultant. Synthesize the data into actionable insights for C-suite.
    Focus on implications, "so what?", and recommendations. Use headings and bullets.

    **Question:** {question}
    **Data:** {result}
    """
    prompt = PromptTemplate.from_template(prompt_template)
    chain = prompt | llm_analyst | StrOutputParser()
    insight = chain.invoke({"question": state['question'], "result": result_str})
    print(f"Generated Insight:\n{insight}")
    return {"insight": insight}


# @title # 5. Graph Definition and Execution
# @markdown Builds the LangGraph with branching for SQL/ML paths.

# --- Tool Nodes: Execute Code ---
def sql_executor_node(state: AgentState) -> AgentState:
    print("---TOOL: SQL Executor---")
    try:
        result_df = execute_sql_query(state['refined_code'])
        return {"result_df": result_df, "error": None}
    except Exception as e:
        print(f"Error: {e}")
        return {"error": str(e)}

def ml_executor_node(state: AgentState) -> AgentState:
    print("---TOOL: ML Executor---")
    try:
        result_df = execute_pyspark_code(state['refined_code'])
        return {"result_df": result_df, "error": None}
    except Exception as e:
        print(f"Error: {e}")
        return {"error": str(e)}

# --- Router Functions ---
def task_router(state: AgentState):
    if state['task_type'] == 'ml':
        return "ml_generation_agent"
    return "sql_generation_agent"

def executor_router(state: AgentState):
    if state['task_type'] == 'ml':
        return "ml_executor"
    return "sql_executor"

# --- Build Graph ---
def build_graph():
    workflow = StateGraph(AgentState)

    # Nodes
    workflow.add_node("planning_agent", planning_agent)
    workflow.add_node("sql_generation_agent", sql_generation_agent)
    workflow.add_node("ml_generation_agent", ml_generation_agent)
    workflow.add_node("code_reviewing_agent", code_reviewing_agent)
    workflow.add_node("sql_executor", sql_executor_node)
    workflow.add_node("ml_executor", ml_executor_node)
    workflow.add_node("insight_analyst_agent", insight_analyst_agent)

    # Edges
    workflow.set_entry_point("planning_agent")
    workflow.add_conditional_edges("planning_agent", task_router, {"ml_generation_agent": "ml_generation_agent", "sql_generation_agent": "sql_generation_agent"})
    workflow.add_edge("sql_generation_agent", "code_reviewing_agent")
    workflow.add_edge("ml_generation_agent", "code_reviewing_agent")
    workflow.add_conditional_edges("code_reviewing_agent", executor_router, {"ml_executor": "ml_executor", "sql_executor": "sql_executor"})
    workflow.add_edge("sql_executor", "insight_analyst_agent")
    workflow.add_edge("ml_executor", "insight_analyst_agent")
    workflow.add_edge("insight_analyst_agent", END)

    graph = workflow.compile()
    print("Graph compiled successfully.")
    return graph

# --- Main Execution ---
def run_analysis(question: str, graph):
    table_schema = get_table_schema(bq_client, BQ_DATASET, BQ_TABLE)
    if "Error" in table_schema:
        print(table_schema)
        return

    initial_state = {"question": question, "schema": table_schema}
    final_state = graph.invoke(initial_state)

    print("\n" + "="*60)
    print("          FINAL BUSINESS INSIGHT & RECOMMENDATIONS")
    print("="*60 + "\n")
    print(final_state.get('insight', 'No insight generated.'))
    print("\n" + "="*60)

    if final_state.get('error'):
        print(f"\nError: {final_state['error']}")

    return final_state


# @title # 6. Run the Analysis
# @markdown Ask your business question; the system handles SQL or ML as needed.

# Build graph
graph = build_graph()

# Ask question
business_question = "Predict sales for the next quarter using linear regression on historical data." # @param {type:"string"}

if business_question:
    run_analysis(business_question, graph)
else:
    print("Please enter a business question in the cell above.")